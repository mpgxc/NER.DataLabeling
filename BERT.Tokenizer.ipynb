{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "following-norway",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "norwegian-shakespeare",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_NAME = 'bert-large-uncased' #'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "finnish-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/kaggle_ner.csv', sep=\",\", encoding=\"latin1\").fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accurate-document",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextNER:\n",
    "\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.__df = df\n",
    "\n",
    "        self.all_words = set(df.Word.values)\n",
    "        self.all_tags = set(df.Tag.values)\n",
    "\n",
    "        self.num_words = len(self.all_words)\n",
    "        self.num_tags = len(self.all_tags)\n",
    "\n",
    "        self.sentences = self.__build_sentences()\n",
    "        self.max_len = self.__get_maxlen()\n",
    "\n",
    "        self.__build_Xy()\n",
    "        self.__build_parsers()\n",
    "\n",
    "    def __get_maxlen(self):\n",
    "        return max([len(x) for x in self.sentences]) \n",
    "\n",
    "    def __build_sentences(self):\n",
    "\n",
    "        return [x for x in self.__df.groupby('Sentence #').apply(\n",
    "            lambda xdef: [x for x in zip(\n",
    "                xdef.Word.values,\n",
    "                xdef.Tag.values\n",
    "            )]\n",
    "        )]\n",
    "\n",
    "    def __build_Xy(self):\n",
    "\n",
    "        self.X = [[word for word, __ in value] for value in self.sentences]\n",
    "        self.y = [[tag for __, tag in value] for value in self.sentences]\n",
    "\n",
    "    def __build_parsers(self):\n",
    "        \n",
    "        self.tag2idx = {value: idx for idx, value in enumerate(self.all_tags)}\n",
    "        self.idx2tag = {idx: value for value, idx in self.tag2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "brief-keeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "contextNER = ContextNER(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adaptive-monday",
   "metadata": {},
   "outputs": [],
   "source": [
    "Words, Tags = contextNER.X, contextNER.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cross-radius",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "falling-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = contextNER.max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "alpine-baltimore",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(sentence, text_labels):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    for word, label in zip(sentence, text_labels):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return  [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"], ['O'] + labels + ['O']\n",
    "\n",
    "def pad_seq(seq, max_seq_length):\n",
    "    return pad_sequences(seq,\n",
    "                         maxlen=max_seq_length,\n",
    "                         dtype=\"long\",\n",
    "                         truncating=\"post\",\n",
    "                         padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "alternative-holocaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts_and_labels = [tokenize_and_preserve_labels(sent, labs) for sent, labs in zip(Words, Tags)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "apart-muslim",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\n",
    "labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "heard-rugby",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, \n",
    "                          dtype=\"long\", \n",
    "                          value=0.0,\n",
    "                          truncating=\"post\", \n",
    "                          padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acquired-cassette",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = pad_sequences([[contextNER.tag2idx.get(l) for l in lab] for lab in labels],\n",
    "                     maxlen=MAX_LEN, \n",
    "                     value=contextNER.tag2idx[\"O\"], \n",
    "                     padding=\"post\",\n",
    "                     dtype=\"long\", \n",
    "                     truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "transsexual-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "executive-vietnamese",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs, tr_tags = input_ids, tags\n",
    "tr_masks = attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "later-court",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "101\t|\t[CLS]\t|\tO\t|\t8\n",
      "2028\t|\tone\t|\tO\t|\t8\n",
      "2001\t|\twas\t|\tO\t|\t8\n",
      "2741\t|\tsent\t|\tO\t|\t8\n",
      "2067\t|\tback\t|\tO\t|\t8\n",
      "2000\t|\tto\t|\tO\t|\t8\n",
      "10411\t|\tsudan\t|\tB-geo\t|\t12\n",
      "1010\t|\t,\t|\tO\t|\t8\n",
      "2178\t|\tanother\t|\tO\t|\t8\n",
      "2000\t|\tto\t|\tO\t|\t8\n",
      "8174\t|\tsaudi\t|\tB-geo\t|\t12\n",
      "9264\t|\tarabia\t|\tI-geo\t|\t1\n",
      "1998\t|\tand\t|\tO\t|\t8\n",
      "1996\t|\tthe\t|\tO\t|\t8\n",
      "2353\t|\tthird\t|\tO\t|\t8\n",
      "2000\t|\tto\t|\tO\t|\t8\n",
      "5207\t|\tjordan\t|\tB-gpe\t|\t0\n",
      "1012\t|\t.\t|\tO\t|\t8\n",
      "102\t|\t[SEP]\t|\tO\t|\t8\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "\n",
    "index_sent = 156\n",
    "\n",
    "for index_word, index_tag in zip(tr_inputs[index_sent], tr_tags[index_sent]):\n",
    "    \n",
    "    word = tokenizer.convert_ids_to_tokens(int(index_word))\n",
    "    tag = contextNER.idx2tag.get(index_tag)\n",
    "    \n",
    "    if index_word != 0:\n",
    "        print(\"{}\\t|\\t{}\\t|\\t{}\\t|\\t{}\".format(index_word, word, tag, index_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dimensional-reality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One\t|\tO\n",
      "was\t|\tO\n",
      "sent\t|\tO\n",
      "back\t|\tO\n",
      "to\t|\tO\n",
      "Sudan\t|\tB-geo\n",
      ",\t|\tO\n",
      "another\t|\tO\n",
      "to\t|\tO\n",
      "Saudi\t|\tB-geo\n",
      "Arabia\t|\tI-geo\n",
      "and\t|\tO\n",
      "the\t|\tO\n",
      "third\t|\tO\n",
      "to\t|\tO\n",
      "Jordan\t|\tB-gpe\n",
      ".\t|\tO\n"
     ]
    }
   ],
   "source": [
    "for w, t in zip(Words[index_sent], Tags[index_sent]):\n",
    "    print(\"{}\\t|\\t{}\".format(w, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "several-memorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessingBERT:\n",
    "    def __init__(self, contextNER):\n",
    "    \n",
    "        self._contextNER = contextNER;\n",
    "\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        \n",
    "        text = self._contextNER.X[item]\n",
    "        tags =  self._contextNER.y[item]\n",
    "\n",
    "        ids = []\n",
    "        target_tag = []\n",
    "\n",
    "        for index, s in enumerate(text):\n",
    "           \n",
    "            inputs = tokenizer.encode(s, add_special_tokens=False)\n",
    "            ids.extend(inputs)\n",
    "\n",
    "            tag = self._contextNER.tag2idx.get(tags[index])\n",
    "            \n",
    "            index_pad = self._contextNER.tag2idx.get('O')\n",
    "            \n",
    "            target_tag.extend([tag] * len(inputs))\n",
    "\n",
    "        ids = ids[:self._contextNER.max_len - 2]\n",
    "        target_tag = target_tag[:self._contextNER.max_len - 2]\n",
    "        \n",
    "        ids = [101] + ids + [102]\n",
    "        target_tag = [index_pad] + target_tag + [index_pad]\n",
    "        \n",
    "        mask = [1] * len(ids)\n",
    "        token_type_ids = [0] * len(ids)\n",
    "        \n",
    "        padding_len = self._contextNER.max_len - len(ids)\n",
    "\n",
    "        ids = ids + ([0] * padding_len)\n",
    "        mask = mask + ([0] * padding_len)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_len)\n",
    "        target_tag = target_tag + ([index_pad] * padding_len)\n",
    "\n",
    "        return {\n",
    "            \"ids\": np.array(ids),\n",
    "            \"mask\": np.array(mask),\n",
    "            \"token_type_ids\": np.array(token_type_ids),\n",
    "            \"target_tag\": np.array(target_tag),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "refined-substitute",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = ProcessingBERT(contextNER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "arranged-barbados",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': array([  101,  5190,  1997, 28337,  2031,  9847,  2083,  2414,  2000,\n",
       "         6186,  1996,  2162,  1999,  5712,  1998,  5157,  1996, 10534,\n",
       "         1997,  2329,  3629,  2013,  2008,  2406,  1012,   102,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0]),\n",
       " 'mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'token_type_ids': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'target_tag': array([ 8,  8,  8,  8,  8,  8,  8, 12,  8,  8,  8,  8,  8, 12,  8,  8,  8,\n",
       "         8,  8,  0,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "         8,  8])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "conscious-marsh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O \t|\t 8 \t|\t O\n",
      "O \t|\t 8 \t|\t O\n",
      "O \t|\t 8 \t|\t O\n",
      "O \t|\t 8 \t|\t O\n",
      "B-tim \t|\t 8 \t|\t O\n",
      "O \t|\t 15 \t|\t B-tim\n",
      "O \t|\t 8 \t|\t O\n",
      "O \t|\t 8 \t|\t O\n",
      "O \t|\t 8 \t|\t O\n",
      "O \t|\t 8 \t|\t O\n",
      "O \t|\t 8 \t|\t O\n",
      "O \t|\t 8 \t|\t O\n",
      "O \t|\t 8 \t|\t O\n",
      "B-geo \t|\t 8 \t|\t O\n",
      "O \t|\t 12 \t|\t B-geo\n",
      "O \t|\t 12 \t|\t B-geo\n",
      "O \t|\t 12 \t|\t B-geo\n",
      "O \t|\t 8 \t|\t O\n",
      "O \t|\t 8 \t|\t O\n",
      "O \t|\t 8 \t|\t O\n",
      "O \t|\t 8 \t|\t O\n",
      "O \t|\t 8 \t|\t O\n",
      "O \t|\t 8 \t|\t O\n",
      "O \t|\t 8 \t|\t O\n",
      "O \t|\t 8 \t|\t O\n",
      "O \t|\t 8 \t|\t O\n",
      "O \t|\t 8 \t|\t O\n",
      "O \t|\t 8 \t|\t O\n",
      "O \t|\t 8 \t|\t O\n",
      "O \t|\t 8 \t|\t O\n",
      "O \t|\t 8 \t|\t O\n",
      "O \t|\t 8 \t|\t O\n",
      "O \t|\t 8 \t|\t O\n",
      "O \t|\t 8 \t|\t O\n",
      "O \t|\t 8 \t|\t O\n",
      "O \t|\t 8 \t|\t O\n"
     ]
    }
   ],
   "source": [
    "idx = 908\n",
    "\n",
    "for tagx, index in zip(Tags[idx], cc[idx]['target_tag']):\n",
    "    tag = contextNER.idx2tag.get(index)\n",
    "    print(tagx, '\\t|\\t', index, '\\t|\\t', tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-asthma",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
